{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 28 0 (offset 0)\n",
      "Ignoring wrong pointing object 34 0 (offset 0)\n",
      "Ignoring wrong pointing object 40 0 (offset 0)\n",
      "Ignoring wrong pointing object 42 0 (offset 0)\n",
      "Ignoring wrong pointing object 48 0 (offset 0)\n",
      "Ignoring wrong pointing object 54 0 (offset 0)\n",
      "Ignoring wrong pointing object 65 0 (offset 0)\n",
      "Ignoring wrong pointing object 67 0 (offset 0)\n",
      "Ignoring wrong pointing object 69 0 (offset 0)\n",
      "Ignoring wrong pointing object 83 0 (offset 0)\n",
      "Ignoring wrong pointing object 85 0 (offset 0)\n",
      "Ignoring wrong pointing object 95 0 (offset 0)\n",
      "Ignoring wrong pointing object 156 0 (offset 0)\n",
      "Ignoring wrong pointing object 188 0 (offset 0)\n",
      "Ignoring wrong pointing object 223 0 (offset 0)\n",
      "Ignoring wrong pointing object 269 0 (offset 0)\n",
      "Ignoring wrong pointing object 400 0 (offset 0)\n",
      "Ignoring wrong pointing object 402 0 (offset 0)\n",
      "Ignoring wrong pointing object 483 0 (offset 0)\n",
      "Ignoring wrong pointing object 637 0 (offset 0)\n",
      "Ignoring wrong pointing object 817 0 (offset 0)\n",
      "Ignoring wrong pointing object 989 0 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"example_documents/deep_learning.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(file_path)\n",
    "pages = []\n",
    "for page in loader.lazy_load():\n",
    "    pages.append(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "\n",
    "# the embeddings model wants the API explicitly for some reason\n",
    "load_dotenv()\n",
    "hf_key = os.getenv('HUGGINGFACEHUB_API_TOKEN')\n",
    "\n",
    "embeddings_model = HuggingFaceInferenceAPIEmbeddings(\n",
    "    api_key=hf_key, model_name=\"sentence-transformers/all-MiniLM-l6-v2\"\n",
    ")\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vector_store = FAISS.from_documents(documents=all_splits, embedding=embeddings_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/danielsuarez-mash/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    max_new_tokens=1000,\n",
    "    top_k=10,\n",
    "    top_p=0.95,\n",
    "    typical_p=0.95,\n",
    "    temperature=0.01,\n",
    "    repetition_penalty=1.03,\n",
    ")\n",
    "\n",
    "chat_model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# History Aware Retriever\n",
    "\n",
    "Takes 3 things:\n",
    "- Query\n",
    "- Chat history\n",
    "- Retriever\n",
    "\n",
    "This chain takes the query and chat history and reformulates a new 'contextualised' query which is then used with the retriever to find relevant documents. These documents are then the output upon invokation. It can be thought of as a replacement for a standard retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "history_aware_retriever = contextualize_q_prompt | chat_model.bind(max_tokens=1000, temperature=1) | StrOutputParser() | retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatHuggingFace] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: Given a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.\\nHuman: Hello, my name is Daniel and I work as a data scientist.I'd really enjoyed the methodology section of this project document. Especially the part on single-stage models.\\nAI: Hello Daniel, nice to meet you. Sounds interesting.\\nHuman: What observation was made about single-stage models?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatHuggingFace] [334ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"What was the conclusion or insight drawn about single-stage models from the methodology section of the project document?\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"What was the conclusion or insight drawn about single-stage models from the methodology section of the project document?\",\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 21,\n",
      "                \"prompt_tokens\": 128,\n",
      "                \"total_tokens\": 149\n",
      "              },\n",
      "              \"model\": \"\",\n",
      "              \"finish_reason\": \"stop\"\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-e4edd572-e4e5-4c30-be80-3909713e7cde-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 21,\n",
      "      \"prompt_tokens\": 128,\n",
      "      \"total_tokens\": 149\n",
      "    },\n",
      "    \"model\": \"\"\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"What was the conclusion or insight drawn about single-stage models from the methodology section of the project document?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [618ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.tracers import ConsoleCallbackHandler\n",
    "\n",
    "chat_history = [\n",
    "    HumanMessage(content=\n",
    "                 \"Hello, my name is Daniel and I work as a data scientist.\"\n",
    "                 \"I'd really enjoyed the methodology section of this project document. Especially the part on single-stage models.\"\n",
    "                 ),\n",
    "    AIMessage(content=\"Hello Daniel, nice to meet you. Sounds interesting.\"),\n",
    "]\n",
    "\n",
    "input = \"Can you remind me of why that type of model was chosen for the project?\"\n",
    "\n",
    "documents = history_aware_retriever.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\":\"What observation was made about single-stage models?\"\n",
    "    },\n",
    "    config={'callbacks': [ConsoleCallbackHandler()]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'example_documents/deep_learning.pdf', 'page': 17, 'start_index': 77}, page_content='Page 17 of 69 3 Methodology 3.1 Methodological approach This project adopted a prototyping approach to developing the best model possible to meet the objectives set in section 1.3. The goal of prototyping is to evaluate an idea. After analysing the research presented in chapter 2, it was clear that the best approach to this problem involved removing unnecessary details from the imagery to make the CNN focus more on the features related to each SRM. Almost every paper in chapter 2 avoided single-stage models such as the Single Shot Detector (SSD) due to a lack of performance against two-stage models.  Not only did this lead to the decision to create the two-stage models presented this project, but it highlighted the importance of the image pre-processing stage. However, at the time, several ideas for how to execute this objective existed. For this reason, prototyping began on the ROI detector. Prototyping was the necessary approach as it was important to create ROI detectors, each'),\n",
       " Document(metadata={'source': 'example_documents/deep_learning.pdf', 'page': 60, 'start_index': 77}, page_content='Page 60 of 69 The majority of Stage 4 did not take place at all due to the total amount of delays which amounted during the project. Networks were adapted to the application as mentioned in 6.1 and AlexNet did produce competitive results without further modification.   Stage 5 took 1 week longer than expected due to the iterative changes made to the model. Often, as soon as results were gathered, new ideas were implemented into the model which meant experiments had to be run again. For example, during the image segmentation problem shown in section 6.5.3, all CNNs had to be trained and analysed again. It took time to develop a routine for collecting data from models. It was paramount to keep the same conditions for each experiment. This meant occasionally restarting the computer to let it cool down after many sessions of CNN training. These sessions would cause performance of the computer to decrease, and it was then decided not to collect performance measurements for the model'),\n",
       " Document(metadata={'source': 'example_documents/deep_learning.pdf', 'page': 58, 'start_index': 564}, page_content='6 10/07/2020 - 19/07/2020 27/07/2020 – 02/08/2020 • Begin iterative process of improving performance of model • Record iterative improvements • Settle on final iteration • Critically analyse performance and computational requirements 7 20/07/2020 - 31/07/2020 31/07/2020 – 14/08/2020 • Produce first write-up draft 8 31/07/2020 - 14/08/2020 07/08/2020 – 14/08/2020 • Final opportunity for technical ideas or modifications to proposed model  • Consider feedback from supervisor to iteratively improve final project write-up  As shown above, the planned project schedule was adhered to for stage 1. This involved searching for and selecting a suitable dataset which contained good quality imagery and ground truth data. Almost all of the first few literature papers at least partly used this dataset to conduct'),\n",
       " Document(metadata={'source': 'example_documents/deep_learning.pdf', 'page': 60, 'start_index': 1671}, page_content='7.2.2.4.   Stage 7 was delayed by over 1 week but only as a results of previous delays. However, it took 1 week longer than expected. This is because of how difficult it was to write the report in a flowing manner. Having a record of tasks completed along with dates prevented this stage from taking any longer.   Since stage 7 overran by 1 week, final modifications and adjustments had to be made simultaneously with report writing. 8.2 Quality Management Since many models were tested during this project, it was important to create code scripts, CNN objects and datasets specifically for them. This required a clear naming procedure which is')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question answer chain\n",
    "\n",
    "This chain is designed to take 3 things:\n",
    "- Context (documents provided by the retriever)\n",
    "- Chat history\n",
    "- User input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    "    \"You are also given chat history below to help contextualise the question.\"\n",
    "    \"{chat_history}\"\n",
    "    \"{input}\"\n",
    ")\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "    (\"system\", system_prompt)\n",
    "    ]\n",
    ")\n",
    "\n",
    "qa_chain = qa_prompt | chat_model.bind(max_tokens=1000, temperature=1) | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "Let's take the documents created in the previous step, the chat history and the user input to test this chain out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [2ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatHuggingFace] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, say that you don't know. Use three sentences maximum and keep the answer concise.\\n\\n[Document(metadata={'source': 'example_documents/deep_learning.pdf', 'page': 17, 'start_index': 77}, page_content='Page 17 of 69 3 Methodology 3.1 Methodological approach This project adopted a prototyping approach to developing the best model possible to meet the objectives set in section 1.3. The goal of prototyping is to evaluate an idea. After analysing the research presented in chapter 2, it was clear that the best approach to this problem involved removing unnecessary details from the imagery to make the CNN focus more on the features related to each SRM. Almost every paper in chapter 2 avoided single-stage models such as the Single Shot Detector (SSD) due to a lack of performance against two-stage models.  Not only did this lead to the decision to create the two-stage models presented this project, but it highlighted the importance of the image pre-processing stage. However, at the time, several ideas for how to execute this objective existed. For this reason, prototyping began on the ROI detector. Prototyping was the necessary approach as it was important to create ROI detectors, each'), Document(metadata={'source': 'example_documents/deep_learning.pdf', 'page': 60, 'start_index': 77}, page_content='Page 60 of 69 The majority of Stage 4 did not take place at all due to the total amount of delays which amounted during the project. Networks were adapted to the application as mentioned in 6.1 and AlexNet did produce competitive results without further modification.   Stage 5 took 1 week longer than expected due to the iterative changes made to the model. Often, as soon as results were gathered, new ideas were implemented into the model which meant experiments had to be run again. For example, during the image segmentation problem shown in section 6.5.3, all CNNs had to be trained and analysed again. It took time to develop a routine for collecting data from models. It was paramount to keep the same conditions for each experiment. This meant occasionally restarting the computer to let it cool down after many sessions of CNN training. These sessions would cause performance of the computer to decrease, and it was then decided not to collect performance measurements for the model'), Document(metadata={'source': 'example_documents/deep_learning.pdf', 'page': 58, 'start_index': 564}, page_content='6 10/07/2020 - 19/07/2020 27/07/2020 – 02/08/2020 • Begin iterative process of improving performance of model • Record iterative improvements • Settle on final iteration • Critically analyse performance and computational requirements 7 20/07/2020 - 31/07/2020 31/07/2020 – 14/08/2020 • Produce first write-up draft 8 31/07/2020 - 14/08/2020 07/08/2020 – 14/08/2020 • Final opportunity for technical ideas or modifications to proposed model  • Consider feedback from supervisor to iteratively improve final project write-up  As shown above, the planned project schedule was adhered to for stage 1. This involved searching for and selecting a suitable dataset which contained good quality imagery and ground truth data. Almost all of the first few literature papers at least partly used this dataset to conduct'), Document(metadata={'source': 'example_documents/deep_learning.pdf', 'page': 60, 'start_index': 1671}, page_content='7.2.2.4.   Stage 7 was delayed by over 1 week but only as a results of previous delays. However, it took 1 week longer than expected. This is because of how difficult it was to write the report in a flowing manner. Having a record of tasks completed along with dates prevented this stage from taking any longer.   Since stage 7 overran by 1 week, final modifications and adjustments had to be made simultaneously with report writing. 8.2 Quality Management Since many models were tested during this project, it was important to create code scripts, CNN objects and datasets specifically for them. This required a clear naming procedure which is')]You are also given chat history below to help contextualise the question.[HumanMessage(content=\\\"Hello, my name is Daniel and I work as a data scientist.I'd really enjoyed the methodology section of this project document. Especially the part on single-stage models.\\\", additional_kwargs={}, response_metadata={}), AIMessage(content='Hello Daniel, nice to meet you. Sounds interesting.', additional_kwargs={}, response_metadata={})]Can you remind me of why that type of model was chosen for the project?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatHuggingFace] [2.16s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The reason for choosing two-stage models over single-stage models, such as the Single Shot Detector (SSD), was due to the lack of performance of single-stage models compared to two-stage models. Almost every paper analyzed in chapter 2 of the document avoided single-stage models and highlighted the importance of the image pre-processing stage, which led to the decision to create the two-stage models presented in this project.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The reason for choosing two-stage models over single-stage models, such as the Single Shot Detector (SSD), was due to the lack of performance of single-stage models compared to two-stage models. Almost every paper analyzed in chapter 2 of the document avoided single-stage models and highlighted the importance of the image pre-processing stage, which led to the decision to create the two-stage models presented in this project.\",\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 82,\n",
      "                \"prompt_tokens\": 1021,\n",
      "                \"total_tokens\": 1103\n",
      "              },\n",
      "              \"model\": \"\",\n",
      "              \"finish_reason\": \"stop\"\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-f666ceef-4a4b-4089-a13d-48cdbccd3aa4-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 82,\n",
      "      \"prompt_tokens\": 1021,\n",
      "      \"total_tokens\": 1103\n",
      "    },\n",
      "    \"model\": \"\"\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The reason for choosing two-stage models over single-stage models, such as the Single Shot Detector (SSD), was due to the lack of performance of single-stage models compared to two-stage models. Almost every paper analyzed in chapter 2 of the document avoided single-stage models and highlighted the importance of the image pre-processing stage, which led to the decision to create the two-stage models presented in this project.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [2.17s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The reason for choosing two-stage models over single-stage models, such as the Single Shot Detector (SSD), was due to the lack of performance of single-stage models compared to two-stage models. Almost every paper analyzed in chapter 2 of the document avoided single-stage models and highlighted the importance of the image pre-processing stage, which led to the decision to create the two-stage models presented in this project.\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The reason for choosing two-stage models over single-stage models, such as the Single Shot Detector (SSD), was due to the lack of performance of single-stage models compared to two-stage models. Almost every paper analyzed in chapter 2 of the document avoided single-stage models and highlighted the importance of the image pre-processing stage, which led to the decision to create the two-stage models presented in this project.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain.invoke({\"context\": documents, \"chat_history\": chat_history, \"input\": input},\n",
    "    config={'callbacks': [ConsoleCallbackHandler()]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval chain\n",
    "All we need now is to connect these two chains together. What is tricky is that we need the chat history and input to go to two places: (1) the history aware retriever and (2) the question answer chain. This is where we can use the RunnablePassthrough function which, in this case, passes through our chat history and input unchanged and into the QA chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "retrieval_chain = (\n",
    "    {\"context\": history_aware_retriever, \"chat_history\": RunnablePassthrough(), \"input\": RunnablePassthrough()}\n",
    "    | qa_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,chat_history,input>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,chat_history,input> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,chat_history,input> > chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,chat_history,input> > chain:RunnablePassthrough] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,chat_history,input> > chain:RunnablePassthrough] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,chat_history,input> > chain:RunnableSequence > prompt:ChatPromptTemplate] [11ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,chat_history,input> > chain:RunnableSequence > llm:ChatHuggingFace] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: Given a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.\\nHuman: Hello, my name is Daniel and I work as a data scientist.I'd really enjoyed the methodology section of this project document. Especially the part on single-stage models.\\nAI: Hello Daniel, nice to meet you. Sounds interesting.\\nHuman: Can you remind me of why that type of model was chosen for the project?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,chat_history,input> > chain:RunnablePassthrough] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,chat_history,input> > chain:RunnablePassthrough] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,chat_history,input> > chain:RunnableSequence > llm:ChatHuggingFace] [879ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"What motivated the choice of the single-stage model over other possible approaches in this project?\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"What motivated the choice of the single-stage model over other possible approaches in this project?\",\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 18,\n",
      "                \"prompt_tokens\": 135,\n",
      "                \"total_tokens\": 153\n",
      "              },\n",
      "              \"model\": \"\",\n",
      "              \"finish_reason\": \"stop\"\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-4bc0e842-6461-46fd-a2a9-b2aaece618ce-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 18,\n",
      "      \"prompt_tokens\": 135,\n",
      "      \"total_tokens\": 153\n",
      "    },\n",
      "    \"model\": \"\"\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,chat_history,input> > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,chat_history,input> > chain:RunnableSequence > parser:StrOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"What motivated the choice of the single-stage model over other possible approaches in this project?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,chat_history,input> > chain:RunnableSequence] [1.34s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,chat_history,input>] [1.35s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatHuggingFace] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, say that you don't know. Use three sentences maximum and keep the answer concise.\\n\\n[Document(metadata={'source': 'example_documents/deep_learning.pdf', 'page': 17, 'start_index': 77}, page_content='Page 17 of 69 3 Methodology 3.1 Methodological approach This project adopted a prototyping approach to developing the best model possible to meet the objectives set in section 1.3. The goal of prototyping is to evaluate an idea. After analysing the research presented in chapter 2, it was clear that the best approach to this problem involved removing unnecessary details from the imagery to make the CNN focus more on the features related to each SRM. Almost every paper in chapter 2 avoided single-stage models such as the Single Shot Detector (SSD) due to a lack of performance against two-stage models.  Not only did this lead to the decision to create the two-stage models presented this project, but it highlighted the importance of the image pre-processing stage. However, at the time, several ideas for how to execute this objective existed. For this reason, prototyping began on the ROI detector. Prototyping was the necessary approach as it was important to create ROI detectors, each'), Document(metadata={'source': 'example_documents/deep_learning.pdf', 'page': 60, 'start_index': 77}, page_content='Page 60 of 69 The majority of Stage 4 did not take place at all due to the total amount of delays which amounted during the project. Networks were adapted to the application as mentioned in 6.1 and AlexNet did produce competitive results without further modification.   Stage 5 took 1 week longer than expected due to the iterative changes made to the model. Often, as soon as results were gathered, new ideas were implemented into the model which meant experiments had to be run again. For example, during the image segmentation problem shown in section 6.5.3, all CNNs had to be trained and analysed again. It took time to develop a routine for collecting data from models. It was paramount to keep the same conditions for each experiment. This meant occasionally restarting the computer to let it cool down after many sessions of CNN training. These sessions would cause performance of the computer to decrease, and it was then decided not to collect performance measurements for the model'), Document(metadata={'source': 'example_documents/deep_learning.pdf', 'page': 60, 'start_index': 870}, page_content='let it cool down after many sessions of CNN training. These sessions would cause performance of the computer to decrease, and it was then decided not to collect performance measurements for the model until the computer had cooled.  Also, organising the data was more time consuming than expected. It was important not to confuse data between models and so extra time and precaution was given to storing data in safe locations. Screenshots were carefully handled and stored in a location referring to their respective models.   Some of the new ideas and iterations mentioned in the above paragraph are part of stage 6. This is when most of the iterative improvements occurred. The biggest development during stage 6 was the creation of the second-generation ROI_RESIZED detector which was explained in 7.2.2.4.   Stage 7 was delayed by over 1 week but only as a results of previous delays. However, it took 1 week longer than expected. This is because of how difficult it was to write the report in a'), Document(metadata={'source': 'example_documents/deep_learning.pdf', 'page': 58, 'start_index': 564}, page_content='6 10/07/2020 - 19/07/2020 27/07/2020 – 02/08/2020 • Begin iterative process of improving performance of model • Record iterative improvements • Settle on final iteration • Critically analyse performance and computational requirements 7 20/07/2020 - 31/07/2020 31/07/2020 – 14/08/2020 • Produce first write-up draft 8 31/07/2020 - 14/08/2020 07/08/2020 – 14/08/2020 • Final opportunity for technical ideas or modifications to proposed model  • Consider feedback from supervisor to iteratively improve final project write-up  As shown above, the planned project schedule was adhered to for stage 1. This involved searching for and selecting a suitable dataset which contained good quality imagery and ground truth data. Almost all of the first few literature papers at least partly used this dataset to conduct')]You are also given chat history below to help contextualise the question.{'chat_history': [HumanMessage(content=\\\"Hello, my name is Daniel and I work as a data scientist.I'd really enjoyed the methodology section of this project document. Especially the part on single-stage models.\\\", additional_kwargs={}, response_metadata={}), AIMessage(content='Hello Daniel, nice to meet you. Sounds interesting.', additional_kwargs={}, response_metadata={})], 'input': 'Can you remind me of why that type of model was chosen for the project?'}{'chat_history': [HumanMessage(content=\\\"Hello, my name is Daniel and I work as a data scientist.I'd really enjoyed the methodology section of this project document. Especially the part on single-stage models.\\\", additional_kwargs={}, response_metadata={}), AIMessage(content='Hello Daniel, nice to meet you. Sounds interesting.', additional_kwargs={}, response_metadata={})], 'input': 'Can you remind me of why that type of model was chosen for the project?'}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatHuggingFace] [1.57s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The project chose two-stage models instead of single-stage models like Single Shot Detector (SSD) because almost every paper analyzed in chapter 2 showed that two-stage models outperformed single-stage models.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The project chose two-stage models instead of single-stage models like Single Shot Detector (SSD) because almost every paper analyzed in chapter 2 showed that two-stage models outperformed single-stage models.\",\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 41,\n",
      "                \"prompt_tokens\": 1186,\n",
      "                \"total_tokens\": 1227\n",
      "              },\n",
      "              \"model\": \"\",\n",
      "              \"finish_reason\": \"stop\"\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-8c3ce7a8-f3b9-4cf5-ac7e-37b81ff3876b-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 41,\n",
      "      \"prompt_tokens\": 1186,\n",
      "      \"total_tokens\": 1227\n",
      "    },\n",
      "    \"model\": \"\"\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The project chose two-stage models instead of single-stage models like Single Shot Detector (SSD) because almost every paper analyzed in chapter 2 showed that two-stage models outperformed single-stage models.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [2.93s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The project chose two-stage models instead of single-stage models like Single Shot Detector (SSD) because almost every paper analyzed in chapter 2 showed that two-stage models outperformed single-stage models.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "output = retrieval_chain.invoke({\"chat_history\": chat_history, \"input\": input},\n",
    "    config={'callbacks': [ConsoleCallbackHandler()]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The project chose two-stage models instead of single-stage models like Single Shot Detector (SSD) because almost every paper analyzed in chapter 2 showed that two-stage models outperformed single-stage models.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history.extend([HumanMessage(content=input), AIMessage(content=output)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"Hello, my name is Daniel and I work as a data scientist.I'd really enjoyed the methodology section of this project document. Especially the part on single-stage models.\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Hello Daniel, nice to meet you. Sounds interesting.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Can you remind me of why that type of model was chosen for the project?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='The project chose two-stage models instead of single-stage models like Single Shot Detector (SSD) because almost every paper analyzed in chapter 2 showed that two-stage models outperformed single-stage models.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,chat_history,input>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,chat_history,input> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,chat_history,input> > chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,chat_history,input> > chain:RunnableSequence > prompt:ChatPromptTemplate] [2ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,chat_history,input> > chain:RunnablePassthrough] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,chat_history,input> > chain:RunnablePassthrough] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,chat_history,input> > chain:RunnableSequence > llm:ChatHuggingFace] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: Given a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.\\nHuman: Hello, my name is Daniel and I work as a data scientist.I'd really enjoyed the methodology section of this project document. Especially the part on single-stage models.\\nAI: Hello Daniel, nice to meet you. Sounds interesting.\\nHuman: Can you remind me of why that type of model was chosen for the project?\\nAI: The project chose two-stage models instead of single-stage models like Single Shot Detector (SSD) because almost every paper analyzed in chapter 2 showed that two-stage models outperformed single-stage models.\\nHuman: And was anything said about the other type of model?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,chat_history,input> > chain:RunnablePassthrough] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,chat_history,input> > chain:RunnablePassthrough] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,chat_history,input> > chain:RunnableSequence > llm:ChatHuggingFace] [662ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Can you remind me what you're referring to by \\\"the other type of model\\\"?\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Can you remind me what you're referring to by \\\"the other type of model\\\"?\",\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 18,\n",
      "                \"prompt_tokens\": 196,\n",
      "                \"total_tokens\": 214\n",
      "              },\n",
      "              \"model\": \"\",\n",
      "              \"finish_reason\": \"stop\"\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-3342bd41-ffb2-4dca-a5ec-46fa2a86e8da-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 18,\n",
      "      \"prompt_tokens\": 196,\n",
      "      \"total_tokens\": 214\n",
      "    },\n",
      "    \"model\": \"\"\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,chat_history,input> > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,chat_history,input> > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"Can you remind me what you're referring to by \\\"the other type of model\\\"?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,chat_history,input> > chain:RunnableSequence] [1.02s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,chat_history,input>] [1.02s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatHuggingFace] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, say that you don't know. Use three sentences maximum and keep the answer concise.\\n\\n[Document(metadata={'source': 'example_documents/deep_learning.pdf', 'page': 50, 'start_index': 870}, page_content='which makes this example a prime one to investigate the differences between the models. As shown in the classification matrix, this misclassification only occurs happens three times between these two classes. By observing the scores in the bottom left image of Figure 42, the CNN was most confident about leftturn followed by 40 and finally 35.'), Document(metadata={'source': 'example_documents/deep_learning.pdf', 'page': 17, 'start_index': 77}, page_content='Page 17 of 69 3 Methodology 3.1 Methodological approach This project adopted a prototyping approach to developing the best model possible to meet the objectives set in section 1.3. The goal of prototyping is to evaluate an idea. After analysing the research presented in chapter 2, it was clear that the best approach to this problem involved removing unnecessary details from the imagery to make the CNN focus more on the features related to each SRM. Almost every paper in chapter 2 avoided single-stage models such as the Single Shot Detector (SSD) due to a lack of performance against two-stage models.  Not only did this lead to the decision to create the two-stage models presented this project, but it highlighted the importance of the image pre-processing stage. However, at the time, several ideas for how to execute this objective existed. For this reason, prototyping began on the ROI detector. Prototyping was the necessary approach as it was important to create ROI detectors, each'), Document(metadata={'source': 'example_documents/deep_learning.pdf', 'page': 45, 'start_index': 77}, page_content='Page 45 of 69  Table 4 - Model results 7.2.1 Comparison with literature The results shown above show a mixture of over and underachievement compared with results obtained by other research shown in section 2.   Before comparing results, it is important to note that the most comparable piece of literature to this project is that of Touqeer Ahmad et al. (2017). This is because this work was based entirely on the same dataset. In fact, just like this project, only the top 10 most represented classes were tested with. This means that this results in this project are directly comparable to those from Touqeer Ahmad et al..  On the other hand, as mentioned in the literature review, Toan Ming Hoang et al. (2019a) combined several datasets including the one used in this project. Also, this paper went further ROI detector + Colour CNN_SqueezeNet 63.33 28.03 ROI_RESIZED detector + Colour CNN_GoogleNet 69.9 24.41 Traffic Light Detection (TLD) detector + Rttld (Zhenchao Ouyang et al., 2020) 99.7'), Document(metadata={'source': 'example_documents/deep_learning.pdf', 'page': 60, 'start_index': 870}, page_content='let it cool down after many sessions of CNN training. These sessions would cause performance of the computer to decrease, and it was then decided not to collect performance measurements for the model until the computer had cooled.  Also, organising the data was more time consuming than expected. It was important not to confuse data between models and so extra time and precaution was given to storing data in safe locations. Screenshots were carefully handled and stored in a location referring to their respective models.   Some of the new ideas and iterations mentioned in the above paragraph are part of stage 6. This is when most of the iterative improvements occurred. The biggest development during stage 6 was the creation of the second-generation ROI_RESIZED detector which was explained in 7.2.2.4.   Stage 7 was delayed by over 1 week but only as a results of previous delays. However, it took 1 week longer than expected. This is because of how difficult it was to write the report in a')]You are also given chat history below to help contextualise the question.{'chat_history': [HumanMessage(content=\\\"Hello, my name is Daniel and I work as a data scientist.I'd really enjoyed the methodology section of this project document. Especially the part on single-stage models.\\\", additional_kwargs={}, response_metadata={}), AIMessage(content='Hello Daniel, nice to meet you. Sounds interesting.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Can you remind me of why that type of model was chosen for the project?', additional_kwargs={}, response_metadata={}), AIMessage(content='The project chose two-stage models instead of single-stage models like Single Shot Detector (SSD) because almost every paper analyzed in chapter 2 showed that two-stage models outperformed single-stage models.', additional_kwargs={}, response_metadata={})], 'input': 'And was anything said about the other type of model?'}{'chat_history': [HumanMessage(content=\\\"Hello, my name is Daniel and I work as a data scientist.I'd really enjoyed the methodology section of this project document. Especially the part on single-stage models.\\\", additional_kwargs={}, response_metadata={}), AIMessage(content='Hello Daniel, nice to meet you. Sounds interesting.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Can you remind me of why that type of model was chosen for the project?', additional_kwargs={}, response_metadata={}), AIMessage(content='The project chose two-stage models instead of single-stage models like Single Shot Detector (SSD) because almost every paper analyzed in chapter 2 showed that two-stage models outperformed single-stage models.', additional_kwargs={}, response_metadata={})], 'input': 'And was anything said about the other type of model?'}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatHuggingFace] [1.28s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The document mentions the Single Shot Detector (SSD) model as an example of a single-stage model that was avoided due to a lack of performance against two-stage models.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The document mentions the Single Shot Detector (SSD) model as an example of a single-stage model that was avoided due to a lack of performance against two-stage models.\",\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 35,\n",
      "                \"prompt_tokens\": 1238,\n",
      "                \"total_tokens\": 1273\n",
      "              },\n",
      "              \"model\": \"\",\n",
      "              \"finish_reason\": \"stop\"\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-6aea1d58-56b0-40e2-98c4-30298a40f612-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 35,\n",
      "      \"prompt_tokens\": 1238,\n",
      "      \"total_tokens\": 1273\n",
      "    },\n",
      "    \"model\": \"\"\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The document mentions the Single Shot Detector (SSD) model as an example of a single-stage model that was avoided due to a lack of performance against two-stage models.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [2.31s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The document mentions the Single Shot Detector (SSD) model as an example of a single-stage model that was avoided due to a lack of performance against two-stage models.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "input2 = \"And was anything said about the other type of model?\"\n",
    "\n",
    "output = retrieval_chain.invoke({\"chat_history\": chat_history, \"input\": input2},\n",
    "    config={'callbacks': [ConsoleCallbackHandler()]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'HumanMessage' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mchat_history\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'HumanMessage' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "chat_history[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
