{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 28 0 (offset 0)\n",
      "Ignoring wrong pointing object 34 0 (offset 0)\n",
      "Ignoring wrong pointing object 40 0 (offset 0)\n",
      "Ignoring wrong pointing object 42 0 (offset 0)\n",
      "Ignoring wrong pointing object 48 0 (offset 0)\n",
      "Ignoring wrong pointing object 54 0 (offset 0)\n",
      "Ignoring wrong pointing object 65 0 (offset 0)\n",
      "Ignoring wrong pointing object 67 0 (offset 0)\n",
      "Ignoring wrong pointing object 69 0 (offset 0)\n",
      "Ignoring wrong pointing object 83 0 (offset 0)\n",
      "Ignoring wrong pointing object 85 0 (offset 0)\n",
      "Ignoring wrong pointing object 95 0 (offset 0)\n",
      "Ignoring wrong pointing object 156 0 (offset 0)\n",
      "Ignoring wrong pointing object 188 0 (offset 0)\n",
      "Ignoring wrong pointing object 223 0 (offset 0)\n",
      "Ignoring wrong pointing object 269 0 (offset 0)\n",
      "Ignoring wrong pointing object 400 0 (offset 0)\n",
      "Ignoring wrong pointing object 402 0 (offset 0)\n",
      "Ignoring wrong pointing object 483 0 (offset 0)\n",
      "Ignoring wrong pointing object 637 0 (offset 0)\n",
      "Ignoring wrong pointing object 817 0 (offset 0)\n",
      "Ignoring wrong pointing object 989 0 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"example_documents/deep_learning.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(file_path)\n",
    "pages = []\n",
    "for page in loader.lazy_load():\n",
    "    pages.append(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "\n",
    "# the embeddings model wants the API explicitly for some reason\n",
    "load_dotenv()\n",
    "hf_key = os.getenv('HUGGINGFACEHUB_API_TOKEN')\n",
    "\n",
    "embeddings_model = HuggingFaceInferenceAPIEmbeddings(\n",
    "    api_key=hf_key, model_name=\"sentence-transformers/all-MiniLM-l6-v2\"\n",
    ")\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vector_store = FAISS.from_documents(documents=all_splits, embedding=embeddings_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /Users/danielsuarez-mash/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    max_new_tokens=1000,\n",
    "    top_k=10,\n",
    "    top_p=0.95,\n",
    "    typical_p=0.95,\n",
    "    temperature=0.01,\n",
    "    repetition_penalty=1.03,\n",
    ")\n",
    "\n",
    "chat_model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# History Aware Retriever\n",
    "\n",
    "Takes 3 things:\n",
    "- Query\n",
    "- Chat history\n",
    "- Retriever\n",
    "\n",
    "This chain takes the query and chat history and reformulates a new 'contextualised' query which is then used with the retriever to find relevant documents. These documents are then the output upon invokation. It can be thought of as a replacement for a standard retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "history_aware_retriever = contextualize_q_prompt | chat_model.bind(max_tokens=1000, temperature=1) | StrOutputParser() | retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [2ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatHuggingFace] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: Given a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.\\nHuman: Hello, my name is Daniel and I work as a data scientist.I'd really enjoyed the methodology section of this project document. Especially the part on single-stage models.\\nAI: Hello Daniel, nice to meet you. Sounds interesting.\\nHuman: What observation was made about single-stage models?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatHuggingFace] [745ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"What are the key findings or insights from the single-stage models section of the project document you liked?\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"What are the key findings or insights from the single-stage models section of the project document you liked?\",\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 21,\n",
      "                \"prompt_tokens\": 128,\n",
      "                \"total_tokens\": 149\n",
      "              },\n",
      "              \"model\": \"\",\n",
      "              \"finish_reason\": \"stop\"\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-de5abc15-b0d2-4729-9be1-991117446ed7-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 21,\n",
      "      \"prompt_tokens\": 128,\n",
      "      \"total_tokens\": 149\n",
      "    },\n",
      "    \"model\": \"\"\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"What are the key findings or insights from the single-stage models section of the project document you liked?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [1.04s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.tracers import ConsoleCallbackHandler\n",
    "\n",
    "chat_history = [\n",
    "    HumanMessage(content=\n",
    "                 \"Hello, my name is Daniel and I work as a data scientist.\"\n",
    "                 \"I'd really enjoyed the methodology section of this project document. Especially the part on single-stage models.\"\n",
    "                 ),\n",
    "    AIMessage(content=\"Hello Daniel, nice to meet you. Sounds interesting.\"),\n",
    "]\n",
    "\n",
    "input = \"Can you remind me of why that type of model was chosen for the project?\"\n",
    "\n",
    "documents = history_aware_retriever.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\":\"What observation was made about single-stage models?\"\n",
    "    },\n",
    "    config={'callbacks': [ConsoleCallbackHandler()]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'example_documents/deep_learning.pdf', 'page': 58, 'start_index': 564}, page_content='6 10/07/2020 - 19/07/2020 27/07/2020 – 02/08/2020 • Begin iterative process of improving performance of model • Record iterative improvements • Settle on final iteration • Critically analyse performance and computational requirements 7 20/07/2020 - 31/07/2020 31/07/2020 – 14/08/2020 • Produce first write-up draft 8 31/07/2020 - 14/08/2020 07/08/2020 – 14/08/2020 • Final opportunity for technical ideas or modifications to proposed model  • Consider feedback from supervisor to iteratively improve final project write-up  As shown above, the planned project schedule was adhered to for stage 1. This involved searching for and selecting a suitable dataset which contained good quality imagery and ground truth data. Almost all of the first few literature papers at least partly used this dataset to conduct'),\n",
       " Document(metadata={'source': 'example_documents/deep_learning.pdf', 'page': 17, 'start_index': 77}, page_content='Page 17 of 69 3 Methodology 3.1 Methodological approach This project adopted a prototyping approach to developing the best model possible to meet the objectives set in section 1.3. The goal of prototyping is to evaluate an idea. After analysing the research presented in chapter 2, it was clear that the best approach to this problem involved removing unnecessary details from the imagery to make the CNN focus more on the features related to each SRM. Almost every paper in chapter 2 avoided single-stage models such as the Single Shot Detector (SSD) due to a lack of performance against two-stage models.  Not only did this lead to the decision to create the two-stage models presented this project, but it highlighted the importance of the image pre-processing stage. However, at the time, several ideas for how to execute this objective existed. For this reason, prototyping began on the ROI detector. Prototyping was the necessary approach as it was important to create ROI detectors, each'),\n",
       " Document(metadata={'source': 'example_documents/deep_learning.pdf', 'page': 64, 'start_index': 77}, page_content='Page 64 of 69 11 Student Reflections  [ A reflective and critical appraisal of your personal performance, problems encountered and how they were resolved, lessons learnt, what could have been done better or differently, etc. ]  This project is the largest piece of work I have completed during my academic studies. As such, it was difficult to predict a realistic timeline at the beginning. On the other hand, almost all of the planned deliverables were delivered; only bounding boxes, data augmentation and CNN modification were not delivered. However, as mentioned, bounding boxes could not be implemented due to issues with the dataset.  In my opinion, the project was kept thoroughly organised. As mentioned in section 8.2, all scripts, datasets and MATLAB objects were stored safely and in organised folders. On the other hand, more could have been done to record daily progress. This is the weakest area of performance in the project. Although issues, successes, solutions and evidence were'),\n",
       " Document(metadata={'source': 'example_documents/deep_learning.pdf', 'page': 60, 'start_index': 1671}, page_content='7.2.2.4.   Stage 7 was delayed by over 1 week but only as a results of previous delays. However, it took 1 week longer than expected. This is because of how difficult it was to write the report in a flowing manner. Having a record of tasks completed along with dates prevented this stage from taking any longer.   Since stage 7 overran by 1 week, final modifications and adjustments had to be made simultaneously with report writing. 8.2 Quality Management Since many models were tested during this project, it was important to create code scripts, CNN objects and datasets specifically for them. This required a clear naming procedure which is')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question answer chain\n",
    "\n",
    "This chain is designed to take 3 things:\n",
    "- Context (documents provided by the retriever)\n",
    "- Chat history\n",
    "- User input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    "    \"You are also given chat history below to help contextualise the question.\"\n",
    "    \"{chat_history}\"\n",
    "    \"{input}\"\n",
    ")\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "    (\"system\", system_prompt)\n",
    "    ]\n",
    ")\n",
    "\n",
    "qa_chain = qa_prompt | chat_model.bind(max_tokens=1000, temperature=1) | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "Let's take the documents created in the previous step, the chat history and the user input to test this chain out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatHuggingFace] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, say that you don't know. Use three sentences maximum and keep the answer concise.\\n\\n[Document(metadata={'source': 'example_documents/deep_learning.pdf', 'page': 58, 'start_index': 564}, page_content='6 10/07/2020 - 19/07/2020 27/07/2020 – 02/08/2020 • Begin iterative process of improving performance of model • Record iterative improvements • Settle on final iteration • Critically analyse performance and computational requirements 7 20/07/2020 - 31/07/2020 31/07/2020 – 14/08/2020 • Produce first write-up draft 8 31/07/2020 - 14/08/2020 07/08/2020 – 14/08/2020 • Final opportunity for technical ideas or modifications to proposed model  • Consider feedback from supervisor to iteratively improve final project write-up  As shown above, the planned project schedule was adhered to for stage 1. This involved searching for and selecting a suitable dataset which contained good quality imagery and ground truth data. Almost all of the first few literature papers at least partly used this dataset to conduct'), Document(metadata={'source': 'example_documents/deep_learning.pdf', 'page': 17, 'start_index': 77}, page_content='Page 17 of 69 3 Methodology 3.1 Methodological approach This project adopted a prototyping approach to developing the best model possible to meet the objectives set in section 1.3. The goal of prototyping is to evaluate an idea. After analysing the research presented in chapter 2, it was clear that the best approach to this problem involved removing unnecessary details from the imagery to make the CNN focus more on the features related to each SRM. Almost every paper in chapter 2 avoided single-stage models such as the Single Shot Detector (SSD) due to a lack of performance against two-stage models.  Not only did this lead to the decision to create the two-stage models presented this project, but it highlighted the importance of the image pre-processing stage. However, at the time, several ideas for how to execute this objective existed. For this reason, prototyping began on the ROI detector. Prototyping was the necessary approach as it was important to create ROI detectors, each'), Document(metadata={'source': 'example_documents/deep_learning.pdf', 'page': 64, 'start_index': 77}, page_content='Page 64 of 69 11 Student Reflections  [ A reflective and critical appraisal of your personal performance, problems encountered and how they were resolved, lessons learnt, what could have been done better or differently, etc. ]  This project is the largest piece of work I have completed during my academic studies. As such, it was difficult to predict a realistic timeline at the beginning. On the other hand, almost all of the planned deliverables were delivered; only bounding boxes, data augmentation and CNN modification were not delivered. However, as mentioned, bounding boxes could not be implemented due to issues with the dataset.  In my opinion, the project was kept thoroughly organised. As mentioned in section 8.2, all scripts, datasets and MATLAB objects were stored safely and in organised folders. On the other hand, more could have been done to record daily progress. This is the weakest area of performance in the project. Although issues, successes, solutions and evidence were'), Document(metadata={'source': 'example_documents/deep_learning.pdf', 'page': 60, 'start_index': 1671}, page_content='7.2.2.4.   Stage 7 was delayed by over 1 week but only as a results of previous delays. However, it took 1 week longer than expected. This is because of how difficult it was to write the report in a flowing manner. Having a record of tasks completed along with dates prevented this stage from taking any longer.   Since stage 7 overran by 1 week, final modifications and adjustments had to be made simultaneously with report writing. 8.2 Quality Management Since many models were tested during this project, it was important to create code scripts, CNN objects and datasets specifically for them. This required a clear naming procedure which is')]You are also given chat history below to help contextualise the question.[HumanMessage(content=\\\"Hello, my name is Daniel and I work as a data scientist.I'd really enjoyed the methodology section of this project document. Especially the part on single-stage models.\\\", additional_kwargs={}, response_metadata={}), AIMessage(content='Hello Daniel, nice to meet you. Sounds interesting.', additional_kwargs={}, response_metadata={})]Can you remind me of why that type of model was chosen for the project?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatHuggingFace] [3.65s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"According to page 17 of the document, the decision to create two-stage models was made after analyzing the research presented in chapter 2, which showed that two-stage models outperformed single-stage models such as the Single Shot Detector (SSD).\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"According to page 17 of the document, the decision to create two-stage models was made after analyzing the research presented in chapter 2, which showed that two-stage models outperformed single-stage models such as the Single Shot Detector (SSD).\",\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 51,\n",
      "                \"prompt_tokens\": 1019,\n",
      "                \"total_tokens\": 1070\n",
      "              },\n",
      "              \"model\": \"\",\n",
      "              \"finish_reason\": \"stop\"\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-a32b4dbe-0e04-49ba-bfa6-3ac92bdfbe15-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 51,\n",
      "      \"prompt_tokens\": 1019,\n",
      "      \"total_tokens\": 1070\n",
      "    },\n",
      "    \"model\": \"\"\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"According to page 17 of the document, the decision to create two-stage models was made after analyzing the research presented in chapter 2, which showed that two-stage models outperformed single-stage models such as the Single Shot Detector (SSD).\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [3.65s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"According to page 17 of the document, the decision to create two-stage models was made after analyzing the research presented in chapter 2, which showed that two-stage models outperformed single-stage models such as the Single Shot Detector (SSD).\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'According to page 17 of the document, the decision to create two-stage models was made after analyzing the research presented in chapter 2, which showed that two-stage models outperformed single-stage models such as the Single Shot Detector (SSD).'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain.invoke({\"context\": documents, \"chat_history\": chat_history, \"input\": input},\n",
    "    config={'callbacks': [ConsoleCallbackHandler()]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval chain\n",
    "All we need now is to connect these two chains together. What is tricky is that we need the chat history and input to go to two places: (1) the history aware retriever and (2) the question answer chain. This is where we can use the RunnablePassthrough function which, in this case, passes through our chat history and input unchanged and into the QA chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "retrieval_chain = (\n",
    "    {\"context\": history_aware_retriever, \"chat_history\": RunnablePassthrough(), \"input\": RunnablePassthrough()}\n",
    "    | qa_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,chat_history,input>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,chat_history,input> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,chat_history,input> > chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,chat_history,input> > chain:RunnableSequence > prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,chat_history,input> > chain:RunnableSequence > llm:ChatHuggingFace] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: Given a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.\\nHuman: Hello, my name is Daniel and I work as a data scientist.I'd really enjoyed the methodology section of this project document. Especially the part on single-stage models.\\nAI: Hello Daniel, nice to meet you. Sounds interesting.\\nHuman: Can you remind me of why that type of model was chosen for the project?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,chat_history,input> > chain:RunnablePassthrough] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,chat_history,input> > chain:RunnablePassthrough] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,chat_history,input> > chain:RunnablePassthrough] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,chat_history,input> > chain:RunnablePassthrough] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,chat_history,input> > chain:RunnableSequence > llm:ChatHuggingFace] [628ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"What motivated the team to use a single-stage model in this project?\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"What motivated the team to use a single-stage model in this project?\",\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 15,\n",
      "                \"prompt_tokens\": 135,\n",
      "                \"total_tokens\": 150\n",
      "              },\n",
      "              \"model\": \"\",\n",
      "              \"finish_reason\": \"stop\"\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-de54947e-535c-4425-ba1a-6c3ddd069c0a-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 15,\n",
      "      \"prompt_tokens\": 135,\n",
      "      \"total_tokens\": 150\n",
      "    },\n",
      "    \"model\": \"\"\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,chat_history,input> > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,chat_history,input> > chain:RunnableSequence > parser:StrOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"What motivated the team to use a single-stage model in this project?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,chat_history,input> > chain:RunnableSequence] [927ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,chat_history,input>] [929ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatHuggingFace] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, say that you don't know. Use three sentences maximum and keep the answer concise.\\n\\n[Document(metadata={'source': 'example_documents/deep_learning.pdf', 'page': 17, 'start_index': 77}, page_content='Page 17 of 69 3 Methodology 3.1 Methodological approach This project adopted a prototyping approach to developing the best model possible to meet the objectives set in section 1.3. The goal of prototyping is to evaluate an idea. After analysing the research presented in chapter 2, it was clear that the best approach to this problem involved removing unnecessary details from the imagery to make the CNN focus more on the features related to each SRM. Almost every paper in chapter 2 avoided single-stage models such as the Single Shot Detector (SSD) due to a lack of performance against two-stage models.  Not only did this lead to the decision to create the two-stage models presented this project, but it highlighted the importance of the image pre-processing stage. However, at the time, several ideas for how to execute this objective existed. For this reason, prototyping began on the ROI detector. Prototyping was the necessary approach as it was important to create ROI detectors, each'), Document(metadata={'source': 'example_documents/deep_learning.pdf', 'page': 60, 'start_index': 77}, page_content='Page 60 of 69 The majority of Stage 4 did not take place at all due to the total amount of delays which amounted during the project. Networks were adapted to the application as mentioned in 6.1 and AlexNet did produce competitive results without further modification.   Stage 5 took 1 week longer than expected due to the iterative changes made to the model. Often, as soon as results were gathered, new ideas were implemented into the model which meant experiments had to be run again. For example, during the image segmentation problem shown in section 6.5.3, all CNNs had to be trained and analysed again. It took time to develop a routine for collecting data from models. It was paramount to keep the same conditions for each experiment. This meant occasionally restarting the computer to let it cool down after many sessions of CNN training. These sessions would cause performance of the computer to decrease, and it was then decided not to collect performance measurements for the model'), Document(metadata={'source': 'example_documents/deep_learning.pdf', 'page': 58, 'start_index': 564}, page_content='6 10/07/2020 - 19/07/2020 27/07/2020 – 02/08/2020 • Begin iterative process of improving performance of model • Record iterative improvements • Settle on final iteration • Critically analyse performance and computational requirements 7 20/07/2020 - 31/07/2020 31/07/2020 – 14/08/2020 • Produce first write-up draft 8 31/07/2020 - 14/08/2020 07/08/2020 – 14/08/2020 • Final opportunity for technical ideas or modifications to proposed model  • Consider feedback from supervisor to iteratively improve final project write-up  As shown above, the planned project schedule was adhered to for stage 1. This involved searching for and selecting a suitable dataset which contained good quality imagery and ground truth data. Almost all of the first few literature papers at least partly used this dataset to conduct'), Document(metadata={'source': 'example_documents/deep_learning.pdf', 'page': 60, 'start_index': 1671}, page_content='7.2.2.4.   Stage 7 was delayed by over 1 week but only as a results of previous delays. However, it took 1 week longer than expected. This is because of how difficult it was to write the report in a flowing manner. Having a record of tasks completed along with dates prevented this stage from taking any longer.   Since stage 7 overran by 1 week, final modifications and adjustments had to be made simultaneously with report writing. 8.2 Quality Management Since many models were tested during this project, it was important to create code scripts, CNN objects and datasets specifically for them. This required a clear naming procedure which is')]You are also given chat history below to help contextualise the question.{'chat_history': [HumanMessage(content=\\\"Hello, my name is Daniel and I work as a data scientist.I'd really enjoyed the methodology section of this project document. Especially the part on single-stage models.\\\", additional_kwargs={}, response_metadata={}), AIMessage(content='Hello Daniel, nice to meet you. Sounds interesting.', additional_kwargs={}, response_metadata={})], 'input': 'Can you remind me of why that type of model was chosen for the project?'}{'chat_history': [HumanMessage(content=\\\"Hello, my name is Daniel and I work as a data scientist.I'd really enjoyed the methodology section of this project document. Especially the part on single-stage models.\\\", additional_kwargs={}, response_metadata={}), AIMessage(content='Hello Daniel, nice to meet you. Sounds interesting.', additional_kwargs={}, response_metadata={})], 'input': 'Can you remind me of why that type of model was chosen for the project?'}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatHuggingFace] [1.50s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The project chose two-stage models instead of single-stage models like SSD due to a lack of performance against two-stage models, as analyzed in chapter 2. This led to the decision to create the two-stage models presented in the project.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The project chose two-stage models instead of single-stage models like SSD due to a lack of performance against two-stage models, as analyzed in chapter 2. This led to the decision to create the two-stage models presented in the project.\",\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 48,\n",
      "                \"prompt_tokens\": 1124,\n",
      "                \"total_tokens\": 1172\n",
      "              },\n",
      "              \"model\": \"\",\n",
      "              \"finish_reason\": \"stop\"\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-ccbb3930-f155-4dbf-9678-76a5f9bdbde5-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 48,\n",
      "      \"prompt_tokens\": 1124,\n",
      "      \"total_tokens\": 1172\n",
      "    },\n",
      "    \"model\": \"\"\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The project chose two-stage models instead of single-stage models like SSD due to a lack of performance against two-stage models, as analyzed in chapter 2. This led to the decision to create the two-stage models presented in the project.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [2.43s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The project chose two-stage models instead of single-stage models like SSD due to a lack of performance against two-stage models, as analyzed in chapter 2. This led to the decision to create the two-stage models presented in the project.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "output = retrieval_chain.invoke({\"chat_history\": chat_history, \"input\": input},\n",
    "    config={'callbacks': [ConsoleCallbackHandler()]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The project chose two-stage models instead of single-stage models like SSD due to a lack of performance against two-stage models, as analyzed in chapter 2. This led to the decision to create the two-stage models presented in the project.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history.extend([HumanMessage(content=input), AIMessage(content=output)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"Hello, my name is Daniel and I work as a data scientist.I'd really enjoyed the methodology section of this project document. Especially the part on single-stage models.\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Hello Daniel, nice to meet you. Sounds interesting.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Can you remind me of why that type of model was chosen for the project?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='The project chose two-stage models instead of single-stage models like SSD due to a lack of performance against two-stage models, as analyzed in chapter 2. This led to the decision to create the two-stage models presented in the project.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,chat_history,input>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,chat_history,input> > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,chat_history,input> > chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,chat_history,input> > chain:RunnableSequence > prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,chat_history,input> > chain:RunnableSequence > llm:ChatHuggingFace] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: Given a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.\\nHuman: Hello, my name is Daniel and I work as a data scientist.I'd really enjoyed the methodology section of this project document. Especially the part on single-stage models.\\nAI: Hello Daniel, nice to meet you. Sounds interesting.\\nHuman: Can you remind me of why that type of model was chosen for the project?\\nAI: The project chose two-stage models instead of single-stage models like SSD due to a lack of performance against two-stage models, as analyzed in chapter 2. This led to the decision to create the two-stage models presented in the project.\\nHuman: And was anything said about the other type of model?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,chat_history,input> > chain:RunnablePassthrough] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,chat_history,input> > chain:RunnablePassthrough] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,chat_history,input> > chain:RunnablePassthrough] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,chat_history,input> > chain:RunnablePassthrough] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,chat_history,input> > chain:RunnableSequence > llm:ChatHuggingFace] [421ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"What do you mean by \\\"other type of model\\\"?\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"What do you mean by \\\"other type of model\\\"?\",\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 12,\n",
      "                \"prompt_tokens\": 203,\n",
      "                \"total_tokens\": 215\n",
      "              },\n",
      "              \"model\": \"\",\n",
      "              \"finish_reason\": \"stop\"\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-90e9ccf6-5343-4e33-b21b-5bbb45235c34-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 12,\n",
      "      \"prompt_tokens\": 203,\n",
      "      \"total_tokens\": 215\n",
      "    },\n",
      "    \"model\": \"\"\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,chat_history,input> > chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,chat_history,input> > chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"What do you mean by \\\"other type of model\\\"?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,chat_history,input> > chain:RunnableSequence] [729ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,chat_history,input>] [730ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatHuggingFace] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, say that you don't know. Use three sentences maximum and keep the answer concise.\\n\\n[Document(metadata={'source': 'example_documents/deep_learning.pdf', 'page': 17, 'start_index': 77}, page_content='Page 17 of 69 3 Methodology 3.1 Methodological approach This project adopted a prototyping approach to developing the best model possible to meet the objectives set in section 1.3. The goal of prototyping is to evaluate an idea. After analysing the research presented in chapter 2, it was clear that the best approach to this problem involved removing unnecessary details from the imagery to make the CNN focus more on the features related to each SRM. Almost every paper in chapter 2 avoided single-stage models such as the Single Shot Detector (SSD) due to a lack of performance against two-stage models.  Not only did this lead to the decision to create the two-stage models presented this project, but it highlighted the importance of the image pre-processing stage. However, at the time, several ideas for how to execute this objective existed. For this reason, prototyping began on the ROI detector. Prototyping was the necessary approach as it was important to create ROI detectors, each'), Document(metadata={'source': 'example_documents/deep_learning.pdf', 'page': 50, 'start_index': 870}, page_content='which makes this example a prime one to investigate the differences between the models. As shown in the classification matrix, this misclassification only occurs happens three times between these two classes. By observing the scores in the bottom left image of Figure 42, the CNN was most confident about leftturn followed by 40 and finally 35.'), Document(metadata={'source': 'example_documents/deep_learning.pdf', 'page': 9, 'start_index': 77}, page_content='Page 9 of 69 2 Literature Review Previous research in this application of detecting or classifying SRMs involve methods which can be split into two main categories: (1) explicitly specifying features such as colour, edges and using machine learning to extract those features in images. And (2), using deep learning to detect more abstract and complex features.   Deep learning methods can be further split into two categories: two-stage such as R-CNN (R. Girshick et al., 2014), Fast R-CNN (R. Girshick, 2015), Faster R-CNN (R. Girshick et al., 2017), Feature Pyramid Networks (FPN) (T.-Y. Lin et al., 2017) and mask R-CNN (K. He et al., 2017) and single-stage models such as the Single Shot Detector (SSD) (W. Liu et al., 2016) and You Only Look Once (YOLO) (J. Redmon and A. Farhadi, 2018). Two-stage models consist of a region proposal unit which pre-processes images in preparation for deep learning architectures. Not knowing what features a deep learning model has learned is what makes deep'), Document(metadata={'source': 'example_documents/deep_learning.pdf', 'page': 9, 'start_index': 880}, page_content='models consist of a region proposal unit which pre-processes images in preparation for deep learning architectures. Not knowing what features a deep learning model has learned is what makes deep learning, especially deep CNNs, a ‘black box’ type of technology. The latest research in the classification and detection of SRMs has begun to involve more and more deep learning.  In relative terms, a much lower amount of research has been conducted in total when compared with research into detecting pedestrians, vehicles, lane markings and traffic lights (Touqeer Ahmad et al., 2017). This is mainly due to the lack of ground truth data available for SRMs. Only after new datasets have emerged such as: Cambridge (Cambridge-Driving Labeled Video Database (CamVid), Oct. 2018), Daimler (Daimler Urban Segmentation Dataset, Jan. 2019), Malaga (The Málaga Stereo and Laser Urban Data Set—MRPT, Oct. 2018), KITTI (A. Geiger et al., 2013) and others, more time been dedicated into this specific')]You are also given chat history below to help contextualise the question.{'chat_history': [HumanMessage(content=\\\"Hello, my name is Daniel and I work as a data scientist.I'd really enjoyed the methodology section of this project document. Especially the part on single-stage models.\\\", additional_kwargs={}, response_metadata={}), AIMessage(content='Hello Daniel, nice to meet you. Sounds interesting.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Can you remind me of why that type of model was chosen for the project?', additional_kwargs={}, response_metadata={}), AIMessage(content='The project chose two-stage models instead of single-stage models like SSD due to a lack of performance against two-stage models, as analyzed in chapter 2. This led to the decision to create the two-stage models presented in the project.', additional_kwargs={}, response_metadata={})], 'input': 'And was anything said about the other type of model?'}{'chat_history': [HumanMessage(content=\\\"Hello, my name is Daniel and I work as a data scientist.I'd really enjoyed the methodology section of this project document. Especially the part on single-stage models.\\\", additional_kwargs={}, response_metadata={}), AIMessage(content='Hello Daniel, nice to meet you. Sounds interesting.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Can you remind me of why that type of model was chosen for the project?', additional_kwargs={}, response_metadata={}), AIMessage(content='The project chose two-stage models instead of single-stage models like SSD due to a lack of performance against two-stage models, as analyzed in chapter 2. This led to the decision to create the two-stage models presented in the project.', additional_kwargs={}, response_metadata={})], 'input': 'And was anything said about the other type of model?'}\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatHuggingFace] [1.56s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The document mentions Single Shot Detector (SSD) as a single-stage model, and explains why it was not chosen for the project. However, it does not provide any further information about SSD or other single-stage models beyond that.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\"\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"The document mentions Single Shot Detector (SSD) as a single-stage model, and explains why it was not chosen for the project. However, it does not provide any further information about SSD or other single-stage models beyond that.\",\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 47,\n",
      "                \"prompt_tokens\": 1290,\n",
      "                \"total_tokens\": 1337\n",
      "              },\n",
      "              \"model\": \"\",\n",
      "              \"finish_reason\": \"stop\"\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-f6e26267-2dbb-44a7-b4c3-314540be13d9-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 47,\n",
      "      \"prompt_tokens\": 1290,\n",
      "      \"total_tokens\": 1337\n",
      "    },\n",
      "    \"model\": \"\"\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The document mentions Single Shot Detector (SSD) as a single-stage model, and explains why it was not chosen for the project. However, it does not provide any further information about SSD or other single-stage models beyond that.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [2.30s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The document mentions Single Shot Detector (SSD) as a single-stage model, and explains why it was not chosen for the project. However, it does not provide any further information about SSD or other single-stage models beyond that.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "input2 = \"And was anything said about the other type of model?\"\n",
    "\n",
    "output = retrieval_chain.invoke({\"chat_history\": chat_history, \"input\": input2},\n",
    "    config={'callbacks': [ConsoleCallbackHandler()]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
